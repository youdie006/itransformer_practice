ã„±import torch
import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.Transformer_EncDec import Encoder, EncoderLayer
from layers.SelfAttention_Family import FullAttention, AttentionLayer
from layers.Embed import DataEmbedding_inverted
import numpy as np


class Model(nn.Module):
    """
    ğŸ”‘ iTransformer í•µì‹¬ ëª¨ë¸: ê¸°ì¡´ Transformerì˜ ì°¨ì›ì„ ë’¤ë°”ê¾¼ êµ¬ì¡°
    Paper link: https://arxiv.org/abs/2310.06625
    
    í•µì‹¬ ì•„ì´ë””ì–´: ì‹œê°„ í† í° â†’ ë³€ìˆ˜ í† í°ìœ¼ë¡œ ë³€í™˜
    - ê¸°ì¡´: (B, L, N) â†’ Lê°œì˜ ì‹œê°„ í† í°ìœ¼ë¡œ ì²˜ë¦¬
    - iTransformer: (B, L, N) â†’ Nê°œì˜ ë³€ìˆ˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬
    """

    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len      # ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 96)
        self.pred_len = configs.pred_len    # ì˜ˆì¸¡ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì˜ˆ: 96, 192, 336, 720)
        self.output_attention = configs.output_attention
        self.use_norm = configs.use_norm    # ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€
        
        # ğŸ”„ í•µì‹¬! DataEmbedding_inverted - ì—¬ê¸°ì„œ ì°¨ì›ì´ ë’¤ë°”ë€œ
        # ì¼ë°˜ Transformer: (B,L,N) â†’ (B,L,d_model)
        # iTransformer: (B,L,N) â†’ (B,N,d_model) 
        self.enc_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,
                                                    configs.dropout)
        self.class_strategy = configs.class_strategy
        
        # ğŸ—ï¸ Encoder-only êµ¬ì¡° (Decoder ì—†ìŒ)
        # Nê°œì˜ ë³€ìˆ˜ í† í°ë“¤ì´ ì„œë¡œ attentioní•˜ì—¬ ìƒê´€ê´€ê³„ í•™ìŠµ
        self.encoder = Encoder(
            [
                EncoderLayer(
                    AttentionLayer(
                        # ğŸ¯ FullAttention: ë³€ìˆ˜ ê°„ ì™„ì „í•œ attention ê³„ì‚°
                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,
                                      output_attention=configs.output_attention), configs.d_model, configs.n_heads),
                    configs.d_model,
                    configs.d_ff,
                    dropout=configs.dropout,
                    activation=configs.activation
                ) for l in range(configs.e_layers)  # e_layersë§Œí¼ ìŒ“ìŒ (ì˜ˆ: 3ì¸µ)
            ],
            norm_layer=torch.nn.LayerNorm(configs.d_model)
        )
        
        # ğŸ“ˆ Projector: d_model â†’ pred_lenìœ¼ë¡œ ë³€í™˜ (ìµœì¢… ì˜ˆì¸¡ ìƒì„±)
        self.projector = nn.Linear(configs.d_model, configs.pred_len, bias=True)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        """
        ğŸ“Š iTransformerì˜ ì˜ˆì¸¡ ê³¼ì • (í•µì‹¬ forward pass)
        
        ì…ë ¥: x_enc (B, L, N) - Batch Ã— ì‹œê°„ê¸¸ì´ Ã— ë³€ìˆ˜ê°œìˆ˜
        ì¶œë ¥: dec_out (B, S, N) - Batch Ã— ì˜ˆì¸¡ê¸¸ì´ Ã— ë³€ìˆ˜ê°œìˆ˜
        """
        
        # ğŸ“ 1ë‹¨ê³„: ì •ê·œí™” (Non-stationary Transformer ê¸°ë²•)
        if self.use_norm:
            means = x_enc.mean(1, keepdim=True).detach()    # ê° ë³€ìˆ˜ë³„ í‰ê·  ê³„ì‚°
            x_enc = x_enc - means                           # í‰ê·  0ìœ¼ë¡œ ë§Œë“¤ê¸°
            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)  # í‘œì¤€í¸ì°¨ ê³„ì‚°
            x_enc /= stdev                                  # í‘œì¤€í¸ì°¨ 1ë¡œ ì •ê·œí™”

        _, _, N = x_enc.shape # B L N
        # B: batch_size    E: d_model(512)
        # L: seq_len(96)   S: pred_len(96,192,336,720) 
        # N: ë³€ìˆ˜ ê°œìˆ˜(ETTh1=7) â† ğŸ”‘ ì´ê²ƒì´ í† í°ì´ ë¨!

        # ğŸ”„ 2ë‹¨ê³„: Inverted Embedding (í•µì‹¬!)
        # ì¼ë°˜ Transformer: (B,L,N) â†’ (B,L,d_model) - Lê°œ ì‹œê°„ í† í°
        # iTransformer: (B,L,N) â†’ (B,N,d_model) - Nê°œ ë³€ìˆ˜ í† í°
        enc_out = self.enc_embedding(x_enc, x_mark_enc) 
        # ë‚´ë¶€ì—ì„œ x.permute(0,2,1) í•´ì„œ (B,L,N) â†’ (B,N,L) â†’ Linear â†’ (B,N,d_model)
        
        # ğŸ¯ 3ë‹¨ê³„: ë³€ìˆ˜ ê°„ Attention (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!)
        # (B,N,d_model) â†’ (B,N,d_model) 
        # NÃ—N attention matrixë¡œ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ í•™ìŠµ
        # ì˜ˆ: ETTh1ì—ì„œ 7Ã—7 attentionìœ¼ë¡œ 7ê°œ ë³€ìˆ˜ê°€ ì„œë¡œ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ í•™ìŠµ
        enc_out, attns = self.encoder(enc_out, attn_mask=None)

        # ğŸ“ˆ 4ë‹¨ê³„: ì˜ˆì¸¡ê°’ ìƒì„± ë° ì°¨ì› ë³µì›
        # (B,N,d_model) â†’ (B,N,pred_len) â†’ (B,pred_len,N)
        dec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :N] # covariates ì œê±°

        # ğŸ“ 5ë‹¨ê³„: ì—­ì •ê·œí™” (ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›)
        if self.use_norm:
            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))  # í‘œì¤€í¸ì°¨ ë³µì›
            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))   # í‰ê·  ë³µì›

        return dec_out, attns


    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        """
        ğŸš€ ë©”ì¸ forward í•¨ìˆ˜ (í›ˆë ¨/ì¶”ë¡  ì‹œ í˜¸ì¶œ)
        
        x_enc: ì…ë ¥ ì‹œê³„ì—´ (B, L, N)
        x_mark_enc: ì‹œê°„ íŠ¹ì„± (ì›”, ì¼, ì‹œê°„ ë“±)
        x_dec, x_mark_dec: iTransformerì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (Encoder-only)
        """
        # forecast í•¨ìˆ˜ì—ì„œ ì‹¤ì œ ì˜ˆì¸¡ ìˆ˜í–‰
        dec_out, attns = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        
        # ìµœì¢… ì˜ˆì¸¡ ê¸¸ì´ë§Œí¼ë§Œ ì˜ë¼ì„œ ë°˜í™˜
        if self.output_attention:
            return dec_out[:, -self.pred_len:, :], attns  # attention weightsë„ í•¨ê»˜ ë°˜í™˜
        else:
            return dec_out[:, -self.pred_len:, :]  # ì˜ˆì¸¡ê°’ë§Œ ë°˜í™˜ [B, pred_len, N]